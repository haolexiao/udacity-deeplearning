{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "3 28 0 None\n",
      "<go> x  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 3 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 3\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  elif char == '<go>':\n",
    "    return GO_ID\n",
    "  elif char == '<eos>':\n",
    "    return EOS_ID\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return \n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 2:\n",
    "    return chr(dictid + first_letter - 3)\n",
    "  elif dictid == GO_ID:\n",
    "    return '<go>'\n",
    "  elif dictid == EOS_ID:\n",
    "    return '<eos>'\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dsa fdsa brn mlv \n"
     ]
    }
   ],
   "source": [
    "def reverse_word(word):\n",
    "  return word[::-1]\n",
    "\n",
    "def mirror_seq(sequence):\n",
    "    return ' '.join(map(lambda x:reverse_word(x),sequence.split(' ')))\n",
    "\n",
    "def mirror_batches(sequence):\n",
    "    return [mirror_seq(seq) for seq in sequence]\n",
    "print(mirror_seq(' asd asdf nrb vlm '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advoc', 'when military govern', 'lleria arches nation', ' abbeys and monaster', 'married urraca princ', 'hel and richard baer', 'y and liturgical lan', 'ay opened for passen', 'tion from the nation', 'migration took place', 'new york other well ', 'he boeing seven six ', 'e listed with a glos', 'eber has probably be', 'o be made to recogni', 'yer who received the', 'ore significant than', 'a fierce critic of t', ' two six eight in si', 'aristotle s uncaused', 'ity can be lost as i', ' and intracellular i', 'tion of the size of ', 'dy to pass him a sti', 'f certain drugs conf', 'at it will take to c', 'e convince the pries', 'ent told him to name', 'ampaign and barred a', 'rver side standard f', 'ious texts such as e', 'o capitalize on the ', 'a duplicate of the o', 'gh ann es d hiver on', 'ine january eight ma', 'ross zero the lead c', 'cal theories classic', 'ast instance the non', ' dimensional analysi', 'most holy mormons be', 't s support or at le', 'u is still disagreed', 'e oscillating system', 'o eight subtypes bas', 'of italy languages t', 's the tower commissi', 'klahoma press one ni', 'erprise linux suse l', 'ws becomes the first', 'et in a nazi concent', 'the fabian society n', 'etchy to relatively ', ' sharman networks sh', 'ised emperor hirohit', 'ting in political in', 'd neo latin most of ', 'th risky riskerdoo r', 'encyclopedic overvie', 'fense the air compon', 'duating from acnm ac', 'treet grid centerlin', 'ations more than any', 'appeal of devotional', 'si have made such de']\n",
      "['sno stsihcrana covda', 'nehw yratilim nrevog', 'airell sehcra noitan', ' syebba dna retsanom', 'deirram acarru cnirp', 'leh dna drahcir reab', 'y dna lacigrutil nal', 'ya denepo rof nessap', 'noit morf eht noitan', 'noitargim koot ecalp', 'wen kroy rehto llew ', 'eh gnieob neves xis ', 'e detsil htiw a solg', 'rebe sah ylbaborp eb', 'o eb edam ot ingocer', 'rey ohw deviecer eht', 'ero tnacifingis naht', 'a ecreif citirc fo t', ' owt xis thgie ni is', 'eltotsira s desuacnu', 'yti nac eb tsol sa i', ' dna ralullecartni i', 'noit fo eht ezis fo ', 'yd ot ssap mih a its', 'f niatrec sgurd fnoc', 'ta ti lliw ekat ot c', 'e ecnivnoc eht seirp', 'tne dlot mih ot eman', 'ngiapma dna derrab a', 'revr edis dradnats f', 'suoi stxet hcus sa e', 'o ezilatipac no eht ', 'a etacilpud fo eht o', 'hg nna se d revih no', 'eni yraunaj thgie am', 'ssor orez eht dael c', 'lac seiroeht cissalc', 'tsa ecnatsni eht non', ' lanoisnemid isylana', 'tsom yloh snomrom eb', 't s troppus ro ta el', 'u si llits deergasid', 'e gnitallicso metsys', 'o thgie sepytbus sab', 'fo ylati segaugnal t', 's eht rewot issimmoc', 'amohalk sserp eno in', 'esirpre xunil esus l', 'sw semoceb eht tsrif', 'te ni a izan tnecnoc', 'eht naibaf yteicos n', 'yhcte ot ylevitaler ', ' namrahs skrowten hs', 'desi rorepme tihorih', 'gnit ni lacitilop ni', 'd oen nital tsom fo ', 'ht yksir oodreksir r', 'cidepolcycne eivrevo', 'esnef eht ria nopmoc', 'gnitaud morf mnca ca', 'teert dirg nilretnec', 'snoita erom naht yna', 'laeppa fo lanoitoved', 'is evah edam hcus ed']\n",
      "[' anarchism originate']\n",
      "['d as a term of abuse']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "sequence_length = 20\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self.sequence_length = sequence_length\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "  \n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batch_char = list()\n",
    "    for b in range(self._batch_size):\n",
    "      batch_char.append( self._text[self._cursor[b]:self._cursor[b]+sequence_length])\n",
    "      self._cursor[b] = (self._cursor[b] + sequence_length) % self._text_size\n",
    "    return batch_char\n",
    "\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "batches = train_batches.next()\n",
    "print(batches)\n",
    "print(mirror_batches(batches))\n",
    "print(valid_batches.next())\n",
    "print(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ons anarchists advoc\n"
     ]
    }
   ],
   "source": [
    "print(train_text[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of weight: 21\n",
      "shape of weight[0]: (?,)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "lstm_size = 64\n",
    "embedding_size = vocabulary_size\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  encoder_inputs = list()\n",
    "  decoder_inputs = list()\n",
    "  train_labels = list()  \n",
    "\n",
    "  for _ in range(sequence_length):\n",
    "    encoder_inputs.append(tf.placeholder(tf.int32, shape=(None,)))\n",
    "  for _ in range(sequence_length+1):\n",
    "    decoder_inputs.append(tf.placeholder(tf.int32, shape=(None,)))\n",
    "    train_labels.append(tf.placeholder(tf.int32, shape=(None,)))\n",
    "\n",
    "  weights = [tf.ones_like(label, dtype=tf.float32) for label in train_labels]\n",
    "\n",
    "  print('length of weight:', len(weights))\n",
    "  print('shape of weight[0]:', weights[0].get_shape())\n",
    "\n",
    "  # Use LSTM cell\n",
    "  cell = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "  #outputs, states = tf.nn.seq2seq.basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)\n",
    "  with tf.variable_scope(\"train_test\"):\n",
    "    outputs, states = tf.nn.seq2seq.embedding_rnn_seq2seq(encoder_inputs,\n",
    "                                                          decoder_inputs,\n",
    "                                                          cell,\n",
    "                                                          vocabulary_size, # num_encoder_symbols\n",
    "                                                          vocabulary_size, # num_decoder_symbols\n",
    "                                                          embedding_size, # embedding_size\n",
    "                                                         )\n",
    "\n",
    "  loss = tf.nn.seq2seq.sequence_loss(outputs, train_labels, weights) \n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "  # Predictions.\n",
    "  train_predictions = tf.pack([tf.nn.softmax(output) for output in outputs])\n",
    "\n",
    "  # Validation eval\n",
    "  valid_encoder_inputs = list()\n",
    "  valid_decoder_inputs = list()\n",
    "  valid_labels = list()  \n",
    "\n",
    "  for _ in range(sequence_length):\n",
    "    valid_encoder_inputs.append(tf.placeholder(tf.int32, shape=(1,)))\n",
    "  for _ in range(sequence_length+1):\n",
    "    valid_decoder_inputs.append(tf.placeholder(tf.int32, shape=(1,)))\n",
    "    valid_labels.append(tf.placeholder(tf.int32, shape=(1,)))\n",
    "  valid_weights = [tf.ones_like(label, dtype=tf.float32) for label in valid_labels]\n",
    "  with tf.variable_scope(\"train_test\", reuse = True):\n",
    "    valid_outputs, valid_states = tf.nn.seq2seq.embedding_rnn_seq2seq(valid_encoder_inputs,\n",
    "                                                                     valid_decoder_inputs,\n",
    "                                                                     cell,\n",
    "                                                                     vocabulary_size, # num_encoder_symbols\n",
    "                                                                     vocabulary_size, # num_decoder_symbols\n",
    "                                                                     embedding_size, # embedding_size\n",
    "                                                                     feed_previous=True\n",
    "                                                                     )\n",
    "  valid_predictions = tf.pack([output for output in valid_outputs])\n",
    "  valid_loss = tf.nn.seq2seq.sequence_loss(valid_outputs, valid_labels, valid_weights) \n",
    "\n",
    "  print(valid_encoder_inputs[0].get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  6.80840760e-02   8.12792629e-02  -1.09792739e-01   3.50007027e-01\n",
      "     6.20396398e-02   5.15164882e-02   1.09323241e-01  -2.46790648e-02\n",
      "     1.05356090e-01  -2.61335313e-01  -8.41471404e-02  -2.73115095e-02\n",
      "    -7.28548393e-02  -1.29638091e-01   1.84192181e-01  -1.71851933e-01\n",
      "     5.64886406e-02   2.53080100e-01  -7.32900053e-02  -3.94653268e-02\n",
      "     3.51401642e-02  -3.41450185e-01   4.07368168e-02  -1.84960023e-01\n",
      "     4.83787619e-04   2.32965529e-01   1.08004443e-01  -2.37516150e-01\n",
      "     1.71504423e-01]]\n",
      "\n",
      " [[  5.96090667e-02   1.09956153e-01  -1.39507309e-01   2.90029913e-01\n",
      "     4.56601903e-02   1.09535996e-02   5.24946228e-02   5.67787327e-02\n",
      "     1.92117579e-02  -2.52704293e-01  -8.92280564e-02  -4.34401892e-02\n",
      "    -7.40001053e-02  -7.55379200e-02   1.64307728e-01  -1.56874120e-01\n",
      "     3.64663228e-02   1.63893953e-01  -1.32927582e-01  -2.86977421e-02\n",
      "     7.59964362e-02  -2.97327340e-01   7.34934658e-02  -1.50578916e-01\n",
      "    -4.92811427e-02   1.54120132e-01   4.08638455e-02  -1.40742317e-01\n",
      "     1.90255210e-01]]\n",
      "\n",
      " [[  6.17275275e-02   1.11260548e-01  -1.54502124e-01   2.36188889e-01\n",
      "     2.95108482e-02  -7.94943422e-03   9.48797259e-03   1.15849204e-01\n",
      "    -4.59465832e-02  -2.28291705e-01  -8.18262100e-02  -4.47497182e-02\n",
      "    -7.98935816e-02  -3.65000181e-02   1.32381409e-01  -1.33931413e-01\n",
      "     2.28170641e-02   8.15031976e-02  -1.76401407e-01  -3.66554521e-02\n",
      "     1.07612334e-01  -2.57616013e-01   9.99124423e-02  -1.18988700e-01\n",
      "    -7.24170953e-02   8.59251544e-02  -1.45127978e-02  -7.24033937e-02\n",
      "     2.03776538e-01]]\n",
      "\n",
      " [[  6.98120669e-02   9.99349207e-02  -1.59323320e-01   1.90297097e-01\n",
      "     1.62406359e-02  -1.18050799e-02  -1.59321688e-02   1.55270204e-01\n",
      "    -9.01500955e-02  -1.93473116e-01  -7.06748962e-02  -4.62191887e-02\n",
      "    -8.28824416e-02  -9.09616798e-03   1.02375984e-01  -1.11411363e-01\n",
      "     1.16169527e-02   2.01108251e-02  -2.11722866e-01  -4.80805822e-02\n",
      "     1.28093109e-01  -2.19865203e-01   1.17401771e-01  -9.42764133e-02\n",
      "    -7.50156343e-02   3.07945367e-02  -5.88100813e-02  -2.67955419e-02\n",
      "     2.14583978e-01]]\n",
      "\n",
      " [[  9.30480585e-02   1.02475621e-01  -9.83719230e-02   1.51776820e-01\n",
      "     2.36214399e-02   1.21366605e-02  -5.46749532e-02   1.15306295e-01\n",
      "    -8.29001367e-02  -1.51347384e-01   1.99957576e-05  -3.62289920e-02\n",
      "    -9.96230096e-02   1.79035868e-02   4.07661945e-02  -1.12083435e-01\n",
      "     4.21166532e-02  -3.03981882e-02  -1.55932426e-01  -2.85299104e-02\n",
      "     1.22861221e-01  -1.82171181e-01   1.49629131e-01  -1.16997831e-01\n",
      "    -6.62874654e-02  -5.31925224e-02  -9.23401117e-02  -2.76768282e-02\n",
      "     1.92461312e-01]]\n",
      "\n",
      " [[  1.12077884e-01   1.02979571e-01  -5.13421372e-02   1.20309353e-01\n",
      "     2.78295893e-02   3.58194932e-02  -6.82346001e-02   7.64080659e-02\n",
      "    -8.04986134e-02  -1.06388420e-01   4.16417681e-02  -2.90520526e-02\n",
      "    -1.13615513e-01   1.89195871e-02   4.41012485e-03  -1.11274168e-01\n",
      "     6.46604449e-02  -6.38840422e-02  -1.20013043e-01  -1.38325375e-02\n",
      "     1.10592507e-01  -1.49139613e-01   1.61089122e-01  -1.29482880e-01\n",
      "    -3.96232828e-02  -1.05919525e-01  -1.14713855e-01  -3.48853841e-02\n",
      "     1.73495606e-01]]\n",
      "\n",
      " [[  1.22666679e-01   1.02693990e-01  -1.08922170e-02   9.73018333e-02\n",
      "     2.84083541e-02   5.67589775e-02  -7.74808377e-02   4.43323590e-02\n",
      "    -7.55994841e-02  -6.94621950e-02   6.38778061e-02  -1.90116093e-02\n",
      "    -1.23760916e-01   1.01842489e-02  -2.03200672e-02  -1.13319710e-01\n",
      "     8.31459016e-02  -8.59137550e-02  -8.80533159e-02  -6.02292037e-03\n",
      "     9.52889100e-02  -1.18365318e-01   1.61757410e-01  -1.37955010e-01\n",
      "    -1.68172009e-02  -1.37486160e-01  -1.29816666e-01  -4.75594625e-02\n",
      "     1.54442191e-01]]\n",
      "\n",
      " [[  4.70413342e-02   2.87024770e-02  -2.18007993e-02   8.69986266e-02\n",
      "     4.32010554e-03   5.70625365e-02  -6.29914477e-02   5.35338111e-02\n",
      "    -3.60059179e-02  -4.03598733e-02   1.16971252e-03  -4.64323722e-03\n",
      "    -8.92254487e-02  -8.34023114e-04  -2.16890201e-02  -5.53603470e-02\n",
      "     5.67065328e-02  -9.42413062e-02  -9.20015723e-02  -3.06218024e-02\n",
      "     9.67126042e-02  -8.81335512e-02   2.02822745e-01  -1.43718690e-01\n",
      "    -1.36353327e-02  -1.53980777e-01  -1.22382209e-01  -6.74648136e-02\n",
      "     1.67703226e-01]]\n",
      "\n",
      " [[ -1.43803749e-03  -2.87321508e-02  -3.39040309e-02   8.59176740e-02\n",
      "    -1.82814281e-02   5.65873086e-02  -6.31691441e-02   6.05747886e-02\n",
      "    -1.13207456e-02  -3.42112221e-02  -4.12807688e-02  -1.31160044e-03\n",
      "    -6.46200404e-02  -6.43931003e-03  -2.69474834e-02  -2.11615115e-02\n",
      "     4.81806584e-02  -1.12204999e-01  -9.44613442e-02  -5.98074645e-02\n",
      "     9.56872553e-02  -6.52481616e-02   2.26016536e-01  -1.46481410e-01\n",
      "    -1.33585837e-02  -1.70957029e-01  -1.14437923e-01  -8.68747681e-02\n",
      "     1.66395679e-01]]\n",
      "\n",
      " [[ -3.07501704e-02  -7.48251006e-02  -4.32822630e-02   8.88354555e-02\n",
      "    -3.88202891e-02   5.90312704e-02  -6.68129623e-02   6.21727817e-02\n",
      "     3.90724558e-03  -3.37380804e-02  -6.88905716e-02  -2.30080984e-03\n",
      "    -4.58323509e-02  -9.54479817e-03  -3.26257050e-02  -1.76122040e-03\n",
      "     4.84276190e-02  -1.28852323e-01  -9.60385278e-02  -8.26559439e-02\n",
      "     9.36172381e-02  -5.14591709e-02   2.38273546e-01  -1.50491014e-01\n",
      "    -1.07828630e-02  -1.88256010e-01  -1.07133038e-01  -1.00511387e-01\n",
      "     1.60211340e-01]]\n",
      "\n",
      " [[ -4.75962795e-02  -1.11440398e-01  -5.07221222e-02   9.31552574e-02\n",
      "    -5.51677905e-02   6.29382282e-02  -6.92303330e-02   6.12582490e-02\n",
      "     1.25225624e-02  -3.61927152e-02  -8.68383422e-02  -5.94108459e-03\n",
      "    -3.17456648e-02  -1.15917902e-02  -3.80624570e-02   8.11799243e-03\n",
      "     5.22723235e-02  -1.41617566e-01  -9.70813185e-02  -9.79655087e-02\n",
      "     9.06946212e-02  -4.42383327e-02   2.42116928e-01  -1.54542983e-01\n",
      "    -6.93680253e-03  -2.04778865e-01  -1.00249730e-01  -1.09302901e-01\n",
      "     1.51869193e-01]]\n",
      "\n",
      " [[ -5.65557629e-02  -1.40285894e-01  -5.66432178e-02   9.76061672e-02\n",
      "    -6.67147338e-02   6.73700348e-02  -6.89804852e-02   5.94976582e-02\n",
      "     1.70088653e-02  -4.00157832e-02  -9.84880254e-02  -1.07820276e-02\n",
      "    -2.15278193e-02  -1.31661296e-02  -4.29055803e-02   1.21699441e-02\n",
      "     5.70756830e-02  -1.49973974e-01  -9.75951552e-02  -1.06323183e-01\n",
      "     8.71752650e-02  -4.16000746e-02   2.40072086e-01  -1.58095405e-01\n",
      "    -2.67472095e-03  -2.19797596e-01  -9.35489759e-02  -1.14321828e-01\n",
      "     1.43007144e-01]]\n",
      "\n",
      " [[ -6.06456399e-02  -1.62866801e-01  -6.12362102e-02   1.01661369e-01\n",
      "    -7.36893564e-02   7.17865229e-02  -6.61320388e-02   5.77294342e-02\n",
      "     1.91770047e-02  -4.44220230e-02  -1.05974056e-01  -1.57485958e-02\n",
      "    -1.44968675e-02  -1.44515578e-02  -4.70814854e-02   1.28915682e-02\n",
      "     6.16628081e-02  -1.54324830e-01  -9.74950045e-02  -1.08975589e-01\n",
      "     8.33743960e-02  -4.21020053e-02   2.34328195e-01  -1.61068603e-01\n",
      "     1.50268455e-03  -2.33002082e-01  -8.69957060e-02  -1.16627000e-01\n",
      "     1.34528235e-01]]\n",
      "\n",
      " [[ -6.17986321e-02  -1.80438265e-01  -6.46526664e-02   1.05108999e-01\n",
      "    -7.67441690e-02   7.59112686e-02  -6.13100007e-02   5.63320331e-02\n",
      "     2.02263426e-02  -4.90582623e-02  -1.10665329e-01  -2.01330576e-02\n",
      "    -1.00419829e-02  -1.54681765e-02  -5.06319702e-02   1.19093936e-02\n",
      "     6.56307563e-02  -1.55407771e-01  -9.67072695e-02  -1.07317194e-01\n",
      "     7.95391500e-02  -4.46796156e-02   2.26583093e-01  -1.63603798e-01\n",
      "     5.33666089e-03  -2.44314790e-01  -8.06633532e-02  -1.17093109e-01\n",
      "     1.26884758e-01]]\n",
      "\n",
      " [[ -6.12296835e-02  -1.94017112e-01  -6.70684949e-02   1.07880123e-01\n",
      "    -7.66954571e-02   7.96311125e-02  -5.52263185e-02   5.54270409e-02\n",
      "     2.08823550e-02  -5.37854247e-02  -1.13446683e-01  -2.35498529e-02\n",
      "    -7.60783534e-03  -1.61841046e-02  -5.36503643e-02   1.02331918e-02\n",
      "     6.89308941e-02  -1.54013917e-01  -9.51928049e-02  -1.02634266e-01\n",
      "     7.58156851e-02  -4.85350192e-02   2.18036830e-01  -1.65888876e-01\n",
      "     8.71534273e-03  -2.53795862e-01  -7.46454597e-02  -1.16376199e-01\n",
      "     1.20272338e-01]]\n",
      "\n",
      " [[ -5.96941635e-02  -2.04417005e-01  -6.86838403e-02   1.09975271e-01\n",
      "    -7.43644834e-02   8.29243585e-02  -4.84861881e-02   5.49967811e-02\n",
      "     2.15380471e-02  -5.85548244e-02  -1.14895277e-01  -2.58591864e-02\n",
      "    -6.70458283e-03  -1.65602192e-02  -5.62480614e-02   8.45149159e-03\n",
      "     7.16470331e-02  -1.50862783e-01  -9.29495618e-02  -9.60013419e-02\n",
      "     7.22653642e-02  -5.30667901e-02   2.09468007e-01  -1.68075904e-01\n",
      "     1.16098598e-02  -2.61585951e-01  -6.90124780e-02  -1.14942022e-01\n",
      "     1.14743859e-01]]\n",
      "\n",
      " [[ -5.76592982e-02  -2.12288052e-01  -6.97011575e-02   1.11429684e-01\n",
      "    -7.04899877e-02   8.58142152e-02  -4.15365659e-02   5.49561046e-02\n",
      "     2.23711394e-02  -6.33436367e-02  -1.15393870e-01  -2.70852149e-02\n",
      "    -6.91771787e-03  -1.65663399e-02  -5.85342050e-02   6.87741861e-03\n",
      "     7.38903284e-02  -1.46557391e-01  -9.00103450e-02  -8.82606953e-02\n",
      "     6.88960999e-02  -5.78264110e-02   2.01337114e-01  -1.70257702e-01\n",
      "     1.40355201e-02  -2.67866403e-01  -6.38010725e-02  -1.13111861e-01\n",
      "     1.10271700e-01]]\n",
      "\n",
      " [[ -5.54133430e-02  -2.18151391e-01  -7.03039095e-02   1.12295702e-01\n",
      "    -6.56887442e-02   8.83412287e-02  -3.46770547e-02   5.51956780e-02\n",
      "     2.34304555e-02  -6.81252033e-02  -1.15203641e-01  -2.73459107e-02\n",
      "    -7.91035499e-03  -1.61868688e-02  -6.06031045e-02   5.65284491e-03\n",
      "     7.57571980e-02  -1.41577676e-01  -8.64392221e-02  -8.00408646e-02\n",
      "     6.56908527e-02  -6.24883920e-02   1.93883672e-01  -1.72473356e-01\n",
      "     1.60297342e-02  -2.72830933e-01  -5.90189248e-02  -1.11105219e-01\n",
      "     1.06781736e-01]]\n",
      "\n",
      " [[ -5.31330332e-02  -2.22426489e-01  -7.06431344e-02   1.12632878e-01\n",
      "    -6.04458041e-02   9.05482769e-02  -2.80915946e-02   5.56076206e-02\n",
      "     2.46943571e-02  -7.28603229e-02  -1.14510082e-01  -2.68016327e-02\n",
      "    -9.41790920e-03  -1.54230483e-02  -6.25276044e-02   4.81886044e-03\n",
      "     7.73181766e-02  -1.36291623e-01  -8.23263004e-02  -7.17906803e-02\n",
      "     6.26277328e-02  -6.68281913e-02   1.87203974e-01  -1.74722537e-01\n",
      "     1.76393799e-02  -2.76667029e-01  -5.46539612e-02  -1.09071434e-01\n",
      "     1.04173794e-01]]\n",
      "\n",
      " [[ -5.09246066e-02  -2.25450680e-01  -7.08328933e-02   1.12503335e-01\n",
      "    -5.51217310e-02   9.24732015e-02  -2.18817201e-02   5.60990386e-02\n",
      "     2.61080507e-02  -7.74988979e-02  -1.13451645e-01  -2.56215092e-02\n",
      "    -1.12387491e-02  -1.42924711e-02  -6.43570796e-02   4.36150283e-03\n",
      "     7.86203519e-02  -1.30971521e-01  -7.77814314e-02  -6.38155416e-02\n",
      "     5.96907027e-02  -7.07030743e-02   1.81308031e-01  -1.76981270e-01\n",
      "     1.89139098e-02  -2.79545665e-01  -5.06831408e-02  -1.07111901e-01\n",
      "     1.02334946e-01]]\n",
      "\n",
      " [[ -4.88493890e-02  -2.27494940e-01  -7.09522814e-02   1.11969076e-01\n",
      "    -4.99687046e-02   9.41460282e-02  -1.60937253e-02   5.65976612e-02\n",
      "     2.76062824e-02  -8.19859132e-02  -1.12136364e-01  -2.39641592e-02\n",
      "    -1.32231405e-02  -1.28274467e-02  -6.61187991e-02   4.23984230e-03\n",
      "     7.96938017e-02  -1.25811607e-01  -7.29264989e-02  -5.63106798e-02\n",
      "     5.68740480e-02  -7.40354061e-02   1.76158696e-01  -1.79214433e-01\n",
      "     1.99015588e-02  -2.81616241e-01  -4.70789149e-02  -1.05294421e-01\n",
      "     1.01149105e-01]]]\n"
     ]
    }
   ],
   "source": [
    "print(valid_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-136-bb3f832567f9>:13 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Step 0:\n",
      "Training set:\n",
      "  Perplexity       :  3.36433\n",
      "  Input            :  irst self labelled a\n",
      "  Correct output   :  tsri fles dellebal a\n",
      "Valid set:\n",
      "  Perplexity       :  3.38995\n",
      "  Input            :  radicals including t\n",
      "  Correct output   :  slacidar gnidulcni t\n",
      "  Generated output :    oooookkzzmmnnnnntrr\n",
      "====================================================================================================\n",
      "Step 1000:\n",
      "Training set:\n",
      "  Perplexity       :  2.68711\n",
      "  Input            :  goldman early french\n",
      "  Correct output   :  namdlog ylrae hcnerf\n",
      "Valid set:\n",
      "  Perplexity       :  2.79355\n",
      "  Input            :  he diggers of the en\n",
      "  Correct output   :  eh sreggid fo eht ne\n",
      "  Generated output :  e e ee ee ene et e<eos><eos><eos>\n",
      "====================================================================================================\n",
      "Step 2000:\n",
      "Training set:\n",
      "  Perplexity       :  2.41827\n",
      "  Input            :  ax stirner the ego a\n",
      "  Correct output   :  xa renrits eht oge a\n",
      "Valid set:\n",
      "  Perplexity       :  3.04977\n",
      "  Input            :  glish revolution and\n",
      "  Correct output   :  hsilg noitulover dna\n",
      "  Generated output :   no nit etit sa sa<eos><eos><eos>\n",
      "====================================================================================================\n",
      "Step 3000:\n",
      "Training set:\n",
      "  Perplexity       :  2.26819\n",
      "  Input            :  one of the following\n",
      "  Correct output   :  eno fo eht gniwollof\n",
      "Valid set:\n",
      "  Perplexity       :  2.19427\n",
      "  Input            :   the sans culottes o\n",
      "  Correct output   :   eht snas settoluc o\n",
      "  Generated output :   eht si sa eht siro<eos><eos>\n",
      "====================================================================================================\n",
      "Step 4000:\n",
      "Training set:\n",
      "  Perplexity       :  2.05776\n",
      "  Input            :  tism spectrum quotie\n",
      "  Correct output   :  msit murtceps eitouq\n",
      "Valid set:\n",
      "  Perplexity       :  4.20596\n",
      "  Input            :  f the french revolut\n",
      "  Correct output   :  f eht hcnerf tulover\n",
      "  Generated output :   eht reht sara enac<eos><eos>\n",
      "====================================================================================================\n",
      "Step 5000:\n",
      "Training set:\n",
      "  Perplexity       :  1.93339\n",
      "  Input            :   alanine and adenosi\n",
      "  Correct output   :   eninala dna isoneda\n",
      "Valid set:\n",
      "  Perplexity       :  3.17083\n",
      "  Input            :  ion whilst the term \n",
      "  Correct output   :  noi tslihw eht mret \n",
      "  Generated output :  siti sa eni eht ra s<eos>\n",
      "====================================================================================================\n",
      "Step 6000:\n",
      "Training set:\n",
      "  Perplexity       :  1.88709\n",
      "  Input            :  es lies in not just \n",
      "  Correct output   :  se seil ni ton tsuj \n",
      "Valid set:\n",
      "  Perplexity       :  2.97087\n",
      "  Input            :  is still used in a p\n",
      "  Correct output   :  si llits desu ni a p\n",
      "  Generated output :  si sil sil dert sna <eos>\n",
      "====================================================================================================\n",
      "Step 7000:\n",
      "Training set:\n",
      "  Perplexity       :  1.56531\n",
      "  Input            :  coln to elite easter\n",
      "  Correct output   :  nloc ot etile retsae\n",
      "Valid set:\n",
      "  Perplexity       :  3.35878\n",
      "  Input            :  ejorative way to des\n",
      "  Correct output   :  evitaroje yaw ot sed\n",
      "  Generated output :  ehtaroc etira sat es<eos>\n",
      "====================================================================================================\n",
      "Step 8000:\n",
      "Training set:\n",
      "  Perplexity       :  1.56111\n",
      "  Input            :  laves he remarked up\n",
      "  Correct output   :  seval eh dekramer pu\n",
      "Valid set:\n",
      "  Perplexity       :  4.6468\n",
      "  Input            :  cribe any act that u\n",
      "  Correct output   :  ebirc yna tca taht u\n",
      "  Generated output :  erit dnal da taht na<eos>\n",
      "====================================================================================================\n",
      "Step 9000:\n",
      "Training set:\n",
      "  Perplexity       :  1.40511\n",
      "  Input            :  storians have ranked\n",
      "  Correct output   :  snairots evah deknar\n",
      "Valid set:\n",
      "  Perplexity       :  3.0459\n",
      "  Input            :  sed violent means to\n",
      "  Correct output   :  des tneloiv snaem ot\n",
      "  Generated output :  des netsemor silam <eos><eos>\n",
      "====================================================================================================\n",
      "Step 10000:\n",
      "Training set:\n",
      "  Perplexity       :  1.40125\n",
      "  Input            :  and aristotle went w\n",
      "  Correct output   :  dna eltotsira tnew w\n",
      "Valid set:\n",
      "  Perplexity       :  4.29655\n",
      "  Input            :   destroy the organiz\n",
      "  Correct output   :   yortsed eht zinagro\n",
      "  Generated output :  s teroht enira dnoht<eos>\n",
      "====================================================================================================\n",
      "Step 11000:\n",
      "Training set:\n",
      "  Perplexity       :  1.20229\n",
      "  Input            :   in the rational sou\n",
      "  Correct output   :   ni eht lanoitar uos\n",
      "Valid set:\n",
      "  Perplexity       :  3.28846\n",
      "  Input            :  ation of society it \n",
      "  Correct output   :  noita fo yteicos ti \n",
      "  Generated output :  oitor eht snitamo t <eos>\n",
      "====================================================================================================\n",
      "Step 12000:\n",
      "Training set:\n",
      "  Perplexity       :  1.14528\n",
      "  Input            :  ed by periodic compa\n",
      "  Correct output   :  de yb cidoirep apmoc\n",
      "Valid set:\n",
      "  Perplexity       :  4.24533\n",
      "  Input            :  has also been taken \n",
      "  Correct output   :  sah osla neeb nekat \n",
      "  Generated output :  ahs sa enolles na tn<eos>\n",
      "====================================================================================================\n",
      "Step 13000:\n",
      "Training set:\n",
      "  Perplexity       :  1.23944\n",
      "  Input            :  effective if one is \n",
      "  Correct output   :  evitceffe fi eno si \n",
      "Valid set:\n",
      "  Perplexity       :  5.41014\n",
      "  Input            :  up as a positive lab\n",
      "  Correct output   :  pu sa a evitisop bal\n",
      "  Generated output :  sa eht nuif cacilaps<eos>\n",
      "====================================================================================================\n",
      "Step 14000:\n",
      "Training set:\n",
      "  Perplexity       :  1.07762\n",
      "  Input            :  ty left in you an ou\n",
      "  Correct output   :  yt tfel ni uoy na uo\n",
      "Valid set:\n",
      "  Perplexity       :  2.45043\n",
      "  Input            :  el by self defined a\n",
      "  Correct output   :  le yb fles denifed a\n",
      "  Generated output :  la eb desul enavef s<eos>\n",
      "====================================================================================================\n",
      "Step 15000:\n",
      "Training set:\n",
      "  Perplexity       :  1.03274\n",
      "  Input            :  eather soon a number\n",
      "  Correct output   :  rehtae noos a rebmun\n",
      "Valid set:\n",
      "  Perplexity       :  2.99507\n",
      "  Input            :  narchists the word a\n",
      "  Correct output   :  stsihcran eht drow a\n",
      "  Generated output :  stsirhca reht nus aw<eos>\n",
      "====================================================================================================\n",
      "Step 16000:\n",
      "Training set:\n",
      "  Perplexity       :  0.930403\n",
      "  Input            :  ne five and in one n\n",
      "  Correct output   :  en evif dna ni eno n\n",
      "Valid set:\n",
      "  Perplexity       :  5.54955\n",
      "  Input            :  narchism is derived \n",
      "  Correct output   :  msihcran si devired \n",
      "  Generated output :  sidnawo di rednie di<eos>\n",
      "====================================================================================================\n",
      "Step 17000:\n",
      "Training set:\n",
      "  Perplexity       :  0.888037\n",
      "  Input            :  recurring absurdity \n",
      "  Correct output   :  gnirrucer ytidrusba \n",
      "Valid set:\n",
      "  Perplexity       :  3.68453\n",
      "  Input            :  from the greek witho\n",
      "  Correct output   :  morf eht keerg ohtiw\n",
      "  Generated output :  orte fo derehp ohgib \n",
      "====================================================================================================\n",
      "Step 18000:\n",
      "Training set:\n",
      "  Perplexity       :  0.778222\n",
      "  Input            :   to run his business\n",
      "  Correct output   :   ot nur sih ssenisub\n",
      "Valid set:\n",
      "  Perplexity       :  2.85778\n",
      "  Input            :  ut archons ruler chi\n",
      "  Correct output   :  tu snohcra relur ihc\n",
      "  Generated output :  ts noruhca reliw dni<eos>\n",
      "====================================================================================================\n",
      "Step 19000:\n",
      "Training set:\n",
      "  Perplexity       :  0.820729\n",
      "  Input            :  ne six one francisco\n",
      "  Correct output   :  en xis eno ocsicnarf\n",
      "Valid set:\n",
      "  Perplexity       :  4.51584\n",
      "  Input            :  ef king anarchism as\n",
      "  Correct output   :  fe gnik msihcrana sa\n",
      "  Generated output :  ge na gnilatuop sadn<eos>\n",
      "====================================================================================================\n",
      "Step 20000:\n",
      "Training set:\n",
      "  Perplexity       :  0.7412\n",
      "  Input            :  ree two five six fou\n",
      "  Correct output   :  eer owt evif xis uof\n",
      "Valid set:\n",
      "  Perplexity       :  3.38268\n",
      "  Input            :   a political philoso\n",
      "  Correct output   :   a lacitilop osolihp\n",
      "  Generated output :   alit laipocif ohsol<eos>\n",
      "====================================================================================================\n",
      "Step 21000:\n",
      "Training set:\n",
      "  Perplexity       :  0.71139\n",
      "  Input            :  ciobiology in the la\n",
      "  Correct output   :  ygoloiboic ni eht al\n",
      "Valid set:\n",
      "  Perplexity       :  4.37364\n",
      "  Input            :  phy is the belief th\n",
      "  Correct output   :  yhp si eht feileb ht\n",
      "  Generated output :  yht s ehc fi eliveho<eos>\n",
      "====================================================================================================\n",
      "Step 22000:\n",
      "Training set:\n",
      "  Perplexity       :  0.659879\n",
      "  Input            :   not any significant\n",
      "  Correct output   :   ton yna tnacifingis\n",
      "Valid set:\n",
      "  Perplexity       :  1.778\n",
      "  Input            :  at rulers are unnece\n",
      "  Correct output   :  ta srelur era ecennu\n",
      "  Generated output :  ta srelru eer nucena<eos>\n",
      "====================================================================================================\n",
      "Step 23000:\n",
      "Training set:\n",
      "  Perplexity       :  0.67679\n",
      "  Input            :  gan zibi reservation\n",
      "  Correct output   :  nag ibiz noitavreser\n",
      "Valid set:\n",
      "  Perplexity       :  3.20845\n",
      "  Input            :  ssary and should be \n",
      "  Correct output   :  yrass dna dluohs eb \n",
      "  Generated output :  yras dnas dlohsu eh <eos>\n",
      "====================================================================================================\n",
      "Step 24000:\n",
      "Training set:\n",
      "  Perplexity       :  0.670465\n",
      "  Input            :  nd genetic relations\n",
      "  Correct output   :  dn citeneg snoitaler\n",
      "Valid set:\n",
      "  Perplexity       :  3.29152\n",
      "  Input            :  abolished although t\n",
      "  Correct output   :  dehsiloba hguohtla t\n",
      "  Generated output :  dehsilohg yacoroh ti<eos>\n",
      "====================================================================================================\n",
      "Step 25000:\n",
      "Training set:\n",
      "  Perplexity       :  0.644551\n",
      "  Input            :  limited human reason\n",
      "  Correct output   :  detimil namuh nosaer\n",
      "Valid set:\n",
      "  Perplexity       :  1.76578\n",
      "  Input            :  here are differing i\n",
      "  Correct output   :  ereh era gnireffid i\n",
      "  Generated output :  ereh rez gniarevif t<eos>\n",
      "====================================================================================================\n",
      "Step 26000:\n",
      "Training set:\n",
      "  Perplexity       :  0.910555\n",
      "  Input            :  llance data report t\n",
      "  Correct output   :  ecnall atad troper t\n",
      "Valid set:\n",
      "  Perplexity       :  5.59133\n",
      "  Input            :  nterpretations of wh\n",
      "  Correct output   :  snoitaterpretn fo hw\n",
      "  Generated output :  sitanertsoren fo ht<eos><eos>\n",
      "====================================================================================================\n",
      "Step 27000:\n",
      "Training set:\n",
      "  Perplexity       :  0.528215\n",
      "  Input            :  ner of austria makin\n",
      "  Correct output   :  ren fo airtsua nikam\n",
      "Valid set:\n",
      "  Perplexity       :  1.33987\n",
      "  Input            :  at this means anarch\n",
      "  Correct output   :  ta siht snaem hcrana\n",
      "  Generated output :  ta siht snaem rahcna<eos>\n",
      "====================================================================================================\n",
      "Step 28000:\n",
      "Training set:\n",
      "  Perplexity       :  0.52426\n",
      "  Input            :  boriginal history ar\n",
      "  Correct output   :  lanigirob yrotsih ra\n",
      "Valid set:\n",
      "  Perplexity       :  3.68821\n",
      "  Input            :  ism also refers to r\n",
      "  Correct output   :  msi osla srefer ot r\n",
      "  Generated output :  msi os laresfo ert e<eos>\n",
      "====================================================================================================\n",
      "Step 29000:\n",
      "Training set:\n",
      "  Perplexity       :  0.572738\n",
      "  Input            :  an contact only abou\n",
      "  Correct output   :  na tcatnoc ylno uoba\n",
      "Valid set:\n",
      "  Perplexity       :  3.42273\n",
      "  Input            :  elated social moveme\n",
      "  Correct output   :  detale laicos emevom\n",
      "  Generated output :  detall eacimos evemo<eos>\n",
      "====================================================================================================\n",
      "Step 30000:\n",
      "Training set:\n",
      "  Perplexity       :  0.55028\n",
      "  Input            :   was no distinction \n",
      "  Correct output   :   saw on noitcnitsid \n",
      "Valid set:\n",
      "  Perplexity       :  2.57951\n",
      "  Input            :  nts that advocate th\n",
      "  Correct output   :  stn taht etacovda ht\n",
      "  Generated output :  stn taht etaduoc ht <eos>\n",
      "====================================================================================================\n",
      "Step 31000:\n",
      "Training set:\n",
      "  Perplexity       :  0.468571\n",
      "  Input            :  unicode org chart on\n",
      "  Correct output   :  edocinu gro trahc no\n",
      "Valid set:\n",
      "  Perplexity       :  2.3983\n",
      "  Input            :  e elimination of aut\n",
      "  Correct output   :  e noitanimile fo tua\n",
      "  Generated output :  o neitainemli fo taw<eos>\n",
      "====================================================================================================\n",
      "Step 32000:\n",
      "Training set:\n",
      "  Perplexity       :  0.432881\n",
      "  Input            :  e number of dutch au\n",
      "  Correct output   :  e rebmun fo hctud ua\n",
      "Valid set:\n",
      "  Perplexity       :  3.23184\n",
      "  Input            :  horitarian instituti\n",
      "  Correct output   :  nairatiroh itutitsni\n",
      "  Generated output :  arnoihtara itititsun<eos>\n",
      "====================================================================================================\n",
      "Step 33000:\n",
      "Training set:\n",
      "  Perplexity       :  0.542082\n",
      "  Input            :   copy the real world\n",
      "  Correct output   :   ypoc eht laer dlrow\n",
      "Valid set:\n",
      "  Perplexity       :  3.07181\n",
      "  Input            :  ons particularly the\n",
      "  Correct output   :  sno ylralucitrap eht\n",
      "  Generated output :  sno ylacirtrulpa eht<eos>\n",
      "====================================================================================================\n",
      "Step 34000:\n",
      "Training set:\n",
      "  Perplexity       :  0.415416\n",
      "  Input            :  unts to have been so\n",
      "  Correct output   :  stnu ot evah neeb os\n",
      "Valid set:\n",
      "  Perplexity       :  0.72689\n",
      "  Input            :   state the word anar\n",
      "  Correct output   :   etats eht drow rana\n",
      "  Generated output :   etats eht draw nora<eos>\n",
      "====================================================================================================\n",
      "Step 35000:\n",
      "Training set:\n",
      "  Perplexity       :  0.464676\n",
      "  Input            :  o zero zero six aust\n",
      "  Correct output   :  o orez orez xis tsua\n",
      "Valid set:\n",
      "  Perplexity       :  2.6181\n",
      "  Input            :  chy as most anarchis\n",
      "  Correct output   :  yhc sa tsom sihcrana\n",
      "  Generated output :  yhc sa tsos biratnac<eos>\n",
      "====================================================================================================\n",
      "Step 36000:\n",
      "Training set:\n",
      "  Perplexity       :  0.563889\n",
      "  Input            :  at little of the dat\n",
      "  Correct output   :  ta elttil fo eht tad\n",
      "Valid set:\n",
      "  Perplexity       :  2.97801\n",
      "  Input            :  ts use it does not i\n",
      "  Correct output   :  st esu ti seod ton i\n",
      "  Generated output :  st esu st eitod on s<eos>\n",
      "====================================================================================================\n",
      "Step 37000:\n",
      "Training set:\n",
      "  Perplexity       :  0.416805\n",
      "  Input            :  by x then the sample\n",
      "  Correct output   :  yb x neht eht elpmas\n",
      "Valid set:\n",
      "  Perplexity       :  4.65187\n",
      "  Input            :  mply chaos nihilism \n",
      "  Correct output   :  ylpm soahc msilihin \n",
      "  Generated output :  yllp sato snoibad ic<eos>\n",
      "====================================================================================================\n",
      "Step 38000:\n",
      "Training set:\n",
      "  Perplexity       :  0.426818\n",
      "  Input            :  the largest budget d\n",
      "  Correct output   :  eht tsegral tegdub d\n",
      "Valid set:\n",
      "  Perplexity       :  1.26624\n",
      "  Input            :  or anomie but rather\n",
      "  Correct output   :  ro eimona tub rehtar\n",
      "  Generated output :  ro emiona tub erhtan<eos>\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def train_prediction(predictions):\n",
    "  return ''.join([characters(onehot)[0] for onehot in predictions])\n",
    "\n",
    "def sample_prediction(predictions):\n",
    "  return ''.join([characters(onehot)[0] for onehot in predictions])\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "  sess.run(tf.initialize_all_variables())\n",
    "\n",
    "  for step in range(50001):\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    input_batches = train_batches.next()\n",
    "    output_batches = mirror_batches(input_batches)\n",
    "       \n",
    "    feed_dict[decoder_inputs[0]] = [GO_ID] * batch_size\n",
    "    for i in range(sequence_length):\n",
    "      feed_dict[encoder_inputs[i]] = [char2id(seq[i]) for seq in input_batches]\n",
    "      feed_dict[decoder_inputs[i+1]] = [char2id(seq[i]) for seq in output_batches]\n",
    "      feed_dict[train_labels[i]] = [char2id(seq[i]) for seq in output_batches]\n",
    "    feed_dict[train_labels[sequence_length]] = [EOS_ID] * batch_size\n",
    "    \n",
    "    _, l, predictions = sess.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "    if step % 1000 == 0:\n",
    "      print('Step %d:' % step)\n",
    "      print('Training set:')\n",
    "      print('  Perplexity       : ', l)\n",
    "      print('  Input            : ', input_batches[0])\n",
    "      print('  Correct output   : ', output_batches[0])\n",
    "      print('  Generated output : ', sample_prediction(predictions))\n",
    "          \n",
    "      valid_feed_dict = dict()\n",
    "      valid_input_batches = valid_batches.next()\n",
    "      valid_output_batches = mirror_batches(valid_input_batches)\n",
    "\n",
    "      valid_feed_dict[valid_decoder_inputs[0]] = [GO_ID]\n",
    "      for i in range(sequence_length):\n",
    "        valid_feed_dict[valid_encoder_inputs[i]] = [char2id(valid_input_batches[0][i])]\n",
    "        valid_feed_dict[valid_labels[i]] = [char2id(valid_output_batches[0][i])]\n",
    "      valid_feed_dict[valid_labels[sequence_length]] = [EOS_ID]\n",
    "\n",
    "      valid_l, valid_p = sess.run([valid_loss, valid_predictions], feed_dict=valid_feed_dict)\n",
    "\n",
    "      print('Valid set:')\n",
    "      print('  Perplexity       : ', valid_l)\n",
    "      print('  Input            : ', valid_input_batches[0])\n",
    "      print('  Correct output   : ', valid_output_batches[0])\n",
    "      print('  Generated output : ', sample_prediction(valid_p))\n",
    "      print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "19\n",
      "20\n",
      "20\n",
      "19\n",
      "19\n",
      "20\n",
      "18\n",
      "20\n",
      "19\n",
      "19\n",
      "19\n",
      "20\n",
      "20\n",
      "20\n",
      "19\n",
      "20\n",
      "20\n",
      "19\n",
      "19\n",
      "20\n",
      "20\n",
      "19\n",
      "20\n",
      "20\n",
      "20\n",
      "19\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "19\n",
      "20\n",
      "20\n",
      "19\n",
      "20\n",
      "20\n",
      "18\n",
      "20\n",
      "20\n",
      "20\n",
      "19\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "19\n",
      "20\n",
      "20\n",
      "19\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "19\n",
      "19\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-61cb124a3f91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mchar2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_batches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mchar2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_batches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mchar2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_batches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEOS_ID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "input_batches = train_batches.next()\n",
    "output_batches = mirror_batches(input_batches)\n",
    "feed_dict[decoder_inputs[0]] = [GO_ID] * batch_size\n",
    "for i in range(sequence_length):\n",
    "    #print(i)\n",
    "    if i==0:\n",
    "        for j in input_batches:\n",
    "            print (len(j))\n",
    "        for j in output_batches:\n",
    "            print (len(j))\n",
    "    [char2id(seq[i]) for seq in input_batches]\n",
    "    [char2id(seq[i]) for seq in output_batches]\n",
    "    [char2id(seq[i]) for seq in output_batches]\n",
    "feed_dict[train_labels[sequence_length]] = [EOS_ID] * batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ate social relations', 'ments failed to revi', 'al park photographic', 'ies index sacred des', 'ess of castile daugh', ' h provided a detail', 'guage among jews man', 'gers in december one', 'al media and from pr', ' during the one nine', 'known manufacturers ', 'seven a widebody jet', 's covering some of t', 'en one of the most i', 'ze single acts of me', ' first card from the', ' in jersey and guern', 'he poverty and socia', 'gns of humanity vol ', ' cause so aquinas co', 'n denaturalization a', 'ce formation solutio', 'the input usually me', 'ck to pull him out b', 'usion inability to o', 'omplete an operation', 't of the mistakes of', ' it fort des moines ', 'ttempts by his oppon', 'ormats for mailboxes', 'soteric christianity', 'growing popularity o', 'riginal document fax', 'e nine eight zero on', 'rch eight listing of', 'haracter lieutenant ', 'al mechanics and spe', ' gm comparison maize', 's fundamental applic', 'lieve the configurat', 'ast not parliament s', ' upon by historians ', ' example rlc circuit', 'ed on the whole geno', 'he official language', 'on at this point pre', 'ne three two one one', 'inux enterprise serv', ' daily college newsp', 'ration camp lewis ha', 'ehru wished the econ', 'stiff from flat to t', 'arman s sydney based', 'o to begin negotiati', 'itiatives the lesoth', 'these authors wrote ', 'icky ricardo this cl', 'w of mathematics pre', 'ent of arm is repres', 'credited programs mu', 'e external links bbc', ' other state modern ', ' buddhism especially', 'vices possible the s']\n",
      "['eta laicos snoitaler', 'stnem deliaf ot iver', 'la krap cihpargotohp', 'sei xedni dercas sed', 'sse fo elitsac hguad', 'h dedivorp a liated', 'egaug gnoma swej nam', 'sreg ni rebmeced eno', 'la aidem dna morf rp', 'gnirud eht eno enin', 'nwonk srerutcafunam', 'neves a ydobediw tej', 's gnirevoc emos fo t', 'ne eno fo eht tsom i', 'ez elgnis stca fo em', 'tsrif drac morf eht', 'ni yesrej dna nreug', 'eh ytrevop dna aicos', 'sng fo ytinamuh lov', 'esuac os saniuqa oc', 'n noitazilarutaned a', 'ec noitamrof oitulos', 'eht tupni yllausu em', 'kc ot llup mih tuo b', 'noisu ytilibani ot o', 'etelpmo na noitarepo', 't fo eht sekatsim fo', 'ti trof sed seniom', 'stpmett yb sih noppo', 'stamro rof sexobliam', 'ciretos ytinaitsirhc', 'gniworg ytiralupop o', 'lanigir tnemucod xaf', 'e enin thgie orez no', 'hcr thgie gnitsil fo', 'retcarah tnanetueil', 'la scinahcem dna eps', 'mg nosirapmoc eziam', 's latnemadnuf cilppa', 'eveil eht tarugifnoc', 'tsa ton tnemailrap s', 'nopu yb snairotsih', 'elpmaxe clr tiucric', 'de no eht elohw oneg', 'eh laiciffo egaugnal', 'no ta siht tniop erp', 'en eerht owt eno eno', 'xuni esirpretne vres', 'yliad egelloc pswen', 'noitar pmac siwel ah', 'urhe dehsiw eht noce', 'ffits morf talf ot t', 'namra s yendys desab', 'o ot nigeb itaitogen', 'sevitaiti eht htosel', 'eseht srohtua etorw', 'ykci odracir siht lc', 'w fo scitamehtam erp', 'tne fo mra si serper', 'detiderc smargorp um', 'e lanretxe sknil cbb', 'rehto etats nredom', 'msihddub yllaicepse', 'seciv elbissop eht s']\n"
     ]
    }
   ],
   "source": [
    "print(input_batches)\n",
    "print(output_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
